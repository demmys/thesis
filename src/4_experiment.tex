\chapter{評価}
\label{experiment}

本章では、本研究の提案手法であるLEONの評価を行う。

\section{評価方針}
\label{applicationperformance}

~\ref{solv:requirements}節で示した通り、機能要件は、1. インターネット上に分散された複数の拠点へのLayer 2ネットワークの拡張、
2. 宛先に応じた転送先の選択、3. 遅延の最も小さい経路でのイーサネットフレームの転送、4. 分散して動作すること、の4つである。機能要件に
ついて、LEONと~\ref{rw:pointtopoint}節と~\ref{rw:pointtomulti}節で挙げた既存研究が、どの程度満たしてるかを表~\ref{table:relatedworksandleon}
に示す。

\begin{table}[h]
	\begin{center}
		\caption{LEONと既存研究の比較}
		\begin{tabular}{|l|c|c|c|c|}
			\hline
			機能要件 & GRE/L2TP & VXLAN & N2N & LEON \\
			\hline
			\hline
			1. インターネット上の複数環境への拡張 & × & × & ◯ & ◯ \\
			\hline
			2. 宛先に応じた転送先の選択 & × & ◯ & ◯ & ◯ \\
			\hline
			3. 遅延の最も小さい経路での転送 & × & × & × & ◯ \\
			\hline
			4. 分散して動作する & × & ◯ & × & ◯ \\
			\hline
		\end{tabular}
		\label{table:relatedworksandleon}
	\end{center}
\end{table}

GREやL2TPなどといった一対一型のLayer 2ネットワーク拡張技術は、どの機能要件も満たしていない。
一方、VXLANとN2Nは一対多型のLayer 2ネットワーク拡張技術のため、両者共に機能要件の2は満たしている。しかし、両者共に遅延の最も小さい経路でイーサネット
フレームを機能を転送する機能がないため、機能要件の3を満たしていない。インターネット上に分散した複数の拠点にLayer 2ネットワークを拡張するLEONは、
トンネル終端点間の遅延を計測し、計測結果から遅延の最も小さい経路を計算することができる。そして、イーサネットフレームを転送する際に、遅延の最も
小さい経路で転送することができる。そのため、機能要件の1、2、3を満たす。また、N2NはSupernode
を必要とするため、機能要件の4を満たさない。VXLANとLEONは分散して動作するため、機能要件の4を満たす。
以上より、LEONは機能要件を全て満たしている。

LEONはインターネット上に分散した複数の拠点を用いて展開されたサービスのパフォーマンスを向上させることを目標としている。
これを実現するために、LEONはLayer 2ネットワークのイーサネットフレームを遅延の最も小さい経路で転送をする。これにより、
サービスを構成するコンポーネントの通信パフォーマンスが向上し、サービスのパフォーマンスが向上すると予想される。そこで
本研究では、LEONが与えるサービスのパフォーマンスへの影響を評価する。

しかし、一方でLEONは全てのトンネル終端点が、全てのトンネル終端点までの遅延計測や死活監視を行なっている。また、ブロードキャストフレーム
やコントロールメッセージを全てのトンネル終端点に転送する。そのため、トンネル終端点の台数が増加するとLEONに悪影響を与える
と予想される。そこで本研究では、トンネル終端点の増加がLEONに与える影響の評価も行う。

\section{サービスのパフォーマンス}

本研究ではまず、LEONが遅延の最も小さい経路でイーサネットフレームを転送することによって、サービスのパフォーマンス
に与える影響を評価する。本評価を行うにあたり、インターネット上に分散された複数の拠点を用いて構築されたサービス
の例としてWIDE Cloudを用いる。

WIDE Cloudには、~\ref{wcclatency}項で説明したように、複数の拠点でサービスを構築
した場合、コンポーネントのパフォーマンスが低下するためサービスのパフォーマンスが低下してしまうという問題がある。
低下するパフォーマンスの1つとして、仮想マシンのディスクパフォーマンスの低下が挙げられる。
LEONを利用することにより、ストレージとの通信を行う際の遅延が小さくなるため、ストレージのパフォーマンスが改善されると予想される。
これによって仮想マシンのディスクパフォーマンスが改善されると予想される。

本評価を行うにあたり、以下の2点の計測項目を評価に用いる。

\begin{itemize}
	\item{NFSv3のファイルシステムパフォーマンス}
	\item{仮想マシン内でのLinux Kernelコンパイル所要時間}
\end{itemize}

LEONは遅延の最も小さい経路でイーサネットフレームの転送を行う。そのため、直接通信を行った場合と比べ、LEONを利用した場合の方が
小さい遅延でNFSv3の通信を行うことができる。これにより、NFSv3のパフォーマンスは直接通信した場合とLEONを利用した場合と比べると、
LEONを利用した場合の方がファイルシステムのパフォーマンスが良いと予想される。本評価では、この予想が正しいかを知るため、NFSv3の
パフォーマンスを直接通信行った場合とLEONを利用した場合で計測し、これを評価をするための1つの指標とする。

また、NFSv3のパフォーマンスが改善されることにより、仮想マシンのパフォーマンスも改善させると予想される。そこで、
本評価では同様に、NFSv3の通信を直接行った場合とLEONを利用した場合で、仮想マシン内でのLinux Kernel~\cite{linuxkernel}コンパイルにかかる
時間を計測する。そして、これを評価をするための1つの指標とする。

\subsection{実験環境}
\label{spexpenv}

評価を行うにあたり、Layer 2ネットワーク上で動作するサービスとそれを構成するコンポーネントのパフォーマンスを計測する。
この計測を行うにあたって、実験環境に対して以下の要求が受けられる。

\begin{itemize}
	\item{実インターネットでの計測環境}
	\item{直接通信するよりも小さい遅延で通信することができる経路が存在すること}
\end{itemize}

LEONはインターネット上に分散された複数の拠点に同一のLayer 2ネットワークを拡張するために利用される。このような想定環境で構築された
サービスのパフォーマンスに与える影響を計測するため、本研究では実験環境を実インターネット上に構築し計測を行う。また、本研究では、最も小さい
経路でイーサネットフレームを転送することによる影響を計測するため、実験環境には直接通信するより、他の拠点を経由することにより
小さい遅延で通信することができる経路が存在する必要がある。

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.65]{./img/spexp}
		\caption{サービスパフォーマンス計測に用いた実験環境}
		\label{img:spexp}
	\end{center}
\end{figure}

実験環境に対する要求から、本実験を行うにあたり、インターネット上に分散された3つの拠点に設置された3台のサーバーを利用して実験環境を構築した。
この実験環境のトポロジー図を図~\ref{img:spexp}に示す。また、利用した実機サーバーの仕様を表~\ref{table:spserv}に示す。
更に、それぞれのサーバーで利用したソフトウェアのバージョンを表~\ref{table:spversion}に示す。Server 1はWIDE Projectの藤沢NOCに設置されたサーバーである。
Server 2はKDDIウェブコミュニケーションズ社~\cite{kddiwebcom}が提供しているCloudCore VPSサービス~\cite{cloudcore}を利用した
仮想サーバーである。そして、Server 3はEditNet社~\cite{editnet}による
フレッツ光ネクスト接続サービス~\cite{enflets}~\cite{nttflets}を用いて接続されたサーバーである。
EditNet社のサービスはさくらインターネット社~\cite{sakurainc}のバックボーンを利用している~\cite{enbb}。

実験を行うにあたり、これら3台のサーバーでLEONの実装であるleondを動作させた。WIDE Projectとさくらインターネットは堂島でピアリングを行なっている。
そのため、Server 1とServer 3が直接通信した場合、堂島を経由するために遅延が22 ms以上かかる。一方、WIDE ProjectとKDDIは東京でピアリングを行なっている。
また、KDDIとさくらインターネットも同様に、東京でピアリングを行なっている。そのため、Server 1とServer 3が通信を行うにはServer 2を経由することにより、
直接通信した場合よりも小さい遅延で通信をすることができる。leondはこの経路を自動的に発見し、Server 1からServer 3宛のイーサネットフレームを転送する際には
Server 2を経由させ転送をする。

\begin{table}[h]
\begin{center}
	\caption{サービスパフォーマンス計測に用いたサーバーの仕様}
	\begin{tabular}{|l|l|l|l|}
		\hline
		ホスト & CPU & メモリー & NIC \\
		\hline
		\hline
		Server 1 (藤沢) & Intel Xeon L5520 2.27GHz & 12GB & NetXtreme II BCM5709 \\
		\hline
		Server 2 (東京) & AMD Phenom 9550 2.20GHz& 2GB & virtio \\
		\hline
		Server 3 (東京) & Intel i7 870 2.93GHz & 16GB & Intel 82574L \\
		\hline
	\end{tabular}
	\label{table:spserv}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
	\caption{サービスパフォーマンス計測に用いた各サーバーのソフトウェアバージョン}
	\begin{tabular}{|l|l|l|l|}
		\hline
		サーバー & OS & Linux Kernel & gcc \\
		\hline
		\hline
		Server 1 (藤沢) & Debian GNU/Linux 6.0.6 Squeeze & 3.7.2 & 4.4.5 \\
		\hline
		Server 2 (東京) & Fedora 17			 & 3.7.4-204 & 4.7.2 \\
		\hline
		Server 3 (東京)	& Fedora 15			 & 2.6.43.8-1 & 4.6.3 \\
		\hline
	\end{tabular}
	\label{table:spversion}
\end{center}
\end{table}

これにより、3台のサーバーを用いて、本実験を行うための要求を満たした実験環境を構築した。


\subsection{実験内容}

本評価を行うにあたり、NFSv3のファイルシステムパフォーマンスと仮想マシン内でのLinux Kernel
コンパイル所要時間を計測した。本実験は、NFSv3の通信を直接インターネット上で行った場合と、
LEONを利用して構築されたLayer 2ネットワーク上で行った場合の2通りで計測した。本実験を行うにあたり、
~\ref{spexpenv}節で説明した実験環境において、Server 1をストレージサーバー、Server 3を仮想マシンを
動かすサーバーとした。Server 1のディスク領域をNFSv3を利用してServer 3から利用できるようにした。

NFSv3のファイルシステムパフォーマンスの計測にはBonnie++~\cite{bonnieplusplus}を用いた。Bonnie++はRussel Coker氏に
よって開発されたディスク・ファイルシステムパフォーマンス計測ツールである。本実験では、Server 3上でBonnie++を動作させ、NFSv3の
シーケンシャルアクセスパフォーマンスとランダムアクセスパフォーマンスを計測した。

また、仮想マシン内でのLinux Kernelコンパイル所要時間を計測するために、Server 3上で仮想マシンを動作させた。
仮想マシンを動作させるための仮想化技術にはKernel-based Virtual Machine(=KVM)~\cite{kvm}を利用した。また、仮想マシンの
ディスクイメージはServer 1に記憶されており、NFSv3を経由して仮想マシンに提供される。仮想マシンの内部で
Linux Kernelのコンパイルを行い、Linux Kernelのコンパイルにかかる時間をtimeコマンドを用いて計測した。

\subsection{実験結果}
\label{spexpres}

サービスのパフォーマンス計測としてまず、NFSv3のファイルシステムパフォーマンスを計測した。計測はインターネット上の経路
で直接NFSv3の通信を行った場合とLEONを利用して拡張されたLayer 2ネットワーク上でNFSv3の通信を行った場合の2通りで計測を
行った。シーケンシャルアクセスパフォーマンスの計測結果を表~\ref{nfsseq}、ランダムアクセスパフォーマンスの計測結果を
表~\ref{nfsrand}に示す。計測はそれぞれ3回行い、計測結果は3回の平均値である。

\begin{table}[h]
        \begin{center}
                \caption{NFSv3のシーケンシャルアクセスパフォーマンス}
                \begin{tabular}{|l|l|l|}
                        \hline
                                通信手法 & read (K/sec) & write (K/sec) \\
                        \hline
			\hline
                                Direct & 4537 & 5372 \\
			\hline
				LEON & 4726 & 5198 \\
                        \hline
                \end{tabular}
                \label{nfsseq}
        \end{center}
\end{table}

\begin{table}[h]
        \begin{center}
                \caption{NFSv3のランダムアクセスパフォーマンス}
                \begin{tabular}{|l|l|l|l|l|}
                        \hline
                                通信手法 & seek(/sec) & create(/sec) & info(/sec) & delete(/sec) \\
                        \hline
			\hline
                                Direct & 2985 & 23 & 46 & 46 \\
			\hline
				LEON & 8417 & 65 & 132 & 132\\
                        \hline
                \end{tabular}
                \label{nfsrand}
        \end{center}
\end{table}

インターネット上の経路で直接通信を行った場合のシーケンシャルアクセスパフォーマンスは、読み込み速度が4537 K/sec、
書き込み速度が5372 K/secであった。一方、LEONを利用して拡張されたLayer 2ネットワーク上で通信を行った場合のシーケンシャル
アクセスパフォーマンスは、読み込み速度が4726 K/sec、書き込み速度が5198 K/secであった。この計測結果から、シーケンシャル
アクセスのパフォーマンスは、インターネット上の経路で直接通信を行った場合と、LEONを利用して拡張されたLayer 2ネットワーク上で
通信を行った場合では、大きは変化はないということがわかった。

一方、インターネット上の経路で直接通信を行った場合のランダムアクセスパフォーマンスは、seek操作が毎秒2985回、create操作
が毎秒23回、info操作が毎秒46回、delete操作が毎秒46回という結果となった。LEONを利用して拡張されたLayer 2ネットワーク上で通信を行った場合
では、seek操作が毎秒8417回、create操作が毎秒65回、info操作が毎秒132回、delete操作が毎秒132回であった。この計測結果から、
ランダムアクセスのパフォーマンスは、インターネット上の経路で直接通信を行った場合に比べ、LEONを利用して拡張されたLayer 2ネットワーク上で
通信を行った場合では約3倍のパフォーマンスとなることがわかった。

次に、サービスのパフォーマンス計測として、仮想マシン内でのLinux Kernelのコンパイル所要時間を計測した。その計測結果を表~\ref{kernelcompile}に示す。
インターネット上の経路で直接通信を行った場合のコンパイル所要時間は78.9分であった。一方、LEONを利用して拡張されたLayer 2ネットワーク上で通信を行った場合
のコンパイル所要時間は58.8分であった。LEONを利用して拡張されたLayer 2ネットワーク上で通信を行うことにより、直接通信を行った場合と比べ、コンパイル所要時間
を約20分削減できることがわかった。

\begin{table}[tb]
        \begin{center}
                \caption{Kernelコンパイルの所要時間}
                \begin{tabular}{|l|l|}
                        \hline
                                通信手法 & コンパイル所要時間 (分) \\
                        \hline
			\hline
                                Direct & 78.9 \\
			\hline
				LEON & 58.8\\
                        \hline
                \end{tabular}
                \label{kernelcompile}
        \end{center}
\end{table}

\subsection{考察}

本研究では、LEONがサービスのパフォーマンスに与える影響を評価した。本評価を行うために、インターネット上の経路で
直接通信した場合と、LEONを利用して拡張されたLayer 2ネットワーク上で通信を行った場合のNFSv3のファイルシステムパフォーマンス
と仮想マシン内でのLinux Kernelコンパイル所要時間を計測した。

LEONを利用することによりランダムアクセスパフォーマンスは改善されたが、シーケンシャルアクセスパフォーマンスは改善されなかった。
NFSv3のシーケンシャルアクセスのパフォーマンスはサーバー間の帯域に依存すると考えられる。今回構築した実験環境では、
フレッツ光ネクスト接続サービスを用いてインターネットに接続されたサーバーの帯域が細いため、このサーバーの回線が
帯域のボトルネックとなっている。そのため、シーケンシャルアクセスのパフォーマンスは改善されなかったと考えられる。
一方、NFSv3のランダムアクセスのパフォーマンスはサーバー間の遅延に依存すると考えられる。NFSv3において1秒間に行える
ランダムアクセスの回数は、1秒間にサーバー間で何回NFSv3のパケットをやり取りできるかに比例する。LEONを利用することにより、
サーバー間の遅延は小さくなる。そのため、1秒間にサーバー間でやり取りできるパケット数が大きくなったため、ランダムアクセス
のパフォーマンスが改善されたと考えられる。

NFSv3のランダムアクセスのパフォーマンスが改善されることにより、仮想マシン内でのLinux Kernelコンパイル
所要時間は約20分短縮された。仮想マシン内でのLinux Kernelコンパイル作業では多くのランダムアクセスが生じると
予想される。そのため、NFSv3のランダムアクセスのパフォーマンスが改善されたため、仮想マシン内でより高速な
ランダムアクセスが可能になったため、Linux Kernelコンパイル所要時間が短縮されたと考えられる。

本実験の実験結果から、LEONを利用することにより、サービスのパフォーマンスは改善されるということがわかった。
LEONは遅延の最も小さい経路でイーサネットフレームの転送を行う。これにより、直接通信した場合と比べ、小さい遅延で
のLayer 2ネットワーク上のコンポーネント間の通信を可能とする。そのため、コンポーネントのパフォーマンスが改善され、
サービスのパフォーマンスが改善される。

\section{トンネル終端点の増加による影響}

本研究では次に、トンネル終端点の増加がLEONに与える影響の評価を行う。LEONを利用してLayer 2ネットワークを拡張している
トンネル終端点の台数が増加すると様々な影響が生じると考えられる。生じる影響の1つとして中継を行なっているトンネル終端点
において障害が発生してから、通信が復旧するまでの時間の増加が考えられる。本研究では、このような障害が発生してから通信復旧
までにかかる時間に着目した。

LEONは宛先のトンネル終端点に、遅延の最も小さい経路で、イーサネットフレームを転送する。これを行うため、LEONでは、
~\ref{solv:latencyprotocol}項で説明したような遅延データベースを構築する。そして、遅延データベースを用いて、
Layer 2ネットワークに参加している1つ1つのトンネル終端点までの遅延が最も小さくなる経路を事前に計算し、拡張された
Layer 2ネットワークのトポロジーを作成している。イーサネットフレームはこのトポロジーに基いて転送される。

VXLANとN2Nは遅延の最も小さい経路でイーサネットフレームの転送を行わないため、拡張されたLayer 2ネットワーク
のトポロジーを作成しない。トポロジーを作成しない利点として、Layer 2ネットワークに参加している何れかのトンネル終端点で障害が
発生しても、拡張されたLayer 2ネットワークにおいて障害は発生しないという点が挙げられる。VXLANの場合、何れかのトンネル終端点
で障害が発生しても、IPマルチキャストが正常に動作していればLayer 2ネットワーク上での通信は正常に行える。同様に、N2Nの場合でも、
Supernodeで障害が発生しなければLayer 2ネットワーク上での通信は正常に行える。そのため、トポロジーを作成しないVXLANとN2Nでは、
あるトンネル終端点で発生した障害によって拡張されたLayer 2ネットワークが影響を受けることはない。

一方で、遅延の最も小さい経路でイーサネットフレームの転送を行うためにトポロジーを作成するLEONは、ある終端点で発生した障害によって
拡張されたLayer 2ネットワークが一時的に影響を受ける場合がある。これは、障害が発生したトンネル終端点が、イーサネットフレームの
中継を行なっていた場合ある。中継を行なっているトンネル終端点で障害が発生すると、そのトンネル終端点を経由する経路が全て利用できなくなる。
LEONは遅延計測を行うと同時に、トンネル終端点の死活監視も行なっている。障害を検知すると、障害が発生しているトンネル終端点をトンネル終端点リスト
から削除し、経路の再計算を行う。しかし、LEONではデフォルトで遅延計測を60秒に1回で行なっていて、遅延計測メッセージの応答が2回ない場合に障害発生と判断している。
そのため、障害が発生から検知までに最大120秒を必要とする。検知してトポロジーが収束するまでは、障害が発生したトンネル終端点にイーサネットフレーム
を転送し続けるため、障害が発生したトンネル終端点を経由する通信は宛先に到達できなくなる。

また、LEONでは拡張されたLayer 2ネットワークに参加している全てのトンネル終端点が、それぞれ異なるトポロジーを作成している。
そのため、あるトンネル終端点で障害が発生してからLayer 2ネットワーク上の通信が完全に正常に戻るには、全てのトンネル終端点が障害を検知し、
トポロジーを作成し直す必要がある。つまり、障害が発生してから復旧までかかる時間は、Layer 2ネットワークに参加しているトンネル終端点の台数の影響を受ける可能性があると
考えられる。

そこで本評価では、Layer 2ネットワークに参加しているトンネル終端点の台数が、あるトンネル終端点の障害が発生してからLayer 2ネットワーク
の通信が正常化するまでかかる時間に与える影響を調査する。調査を行うために、トンネル終端点を徐々に増加させ、障害発生から全てのトンネル終端点で
のトポロジー収束までかかる時間を計測する。具体的には、全てのトンネル終端点の通信を中継しているトンネル終端点で障害を発生させ、そのトンネル終端点を
経由していた通信が復旧するまでにかかる時間を計測する。この計測をトンネル終端点の台数を変化させながら行う。

\subsection{実験環境}
\label{experiment:environment}

本研究では、イーサネットフレームの転送を中継しているトンネル終端点で障害が発生した際に、Layer 2ネットワークに参加
しているトンネル終端点の台数が、トポロジーの収束時間に与える影響を評価する。評価を行うにあたり、Layer 2ネットワーク
に参加しているトンネル終端点の台数を変化させ、それぞれの場合での障害発生から
全てのトンネル終端点でトポロジーが収束するまでにかかる時間を計測する。この計測を行うにあたって、実験環境に対して
以下の要求が受けられる。

\begin{itemize}
	\item{全てのトンネル終端点間に遅延が存在すること}
	\item{直接通信するよりも小さい遅延で通信することができる経路が存在すること}
	\item{数十台のトンネル終端点}
\end{itemize}

LEONはインターネット上に分散された複数の拠点に同一のLayer 2ネットワークを拡張する
ために利用される。インターネットにおいて、分散された複数の拠点同士が通信するにあたり、遅延が生じる。そのため、実験環境を
実インターネット環境と類似した環境にするためには、実験環境のトンネル終端点間に遅延が存在する必要がある。

また、本実験では、中継をするトンネル終端点で障害が発生してから、全てのトンネル終端点でトポロジーが収束するまでの時間を計測する。
この計測を行うには、あるトンネル終端点がイーサネットフレームを転送する際に、中継するトンネル終端点が必要となる。LEONは直接
宛先のトンネル終端点へ転送するよりも、他のトンネル終端点を経由して宛先のトンネル終端点へ転送することにより、小さい遅延で転送できるような
経路が存在した場合に中継するトンネル終端点を設定する。そのため、実験環境では、直接通信するよりも小さい遅延で通信することができる経路が必要となる。

さらに、~\ref{solv:env}節で説明したように、LEONは数十の拠点にLayer 2ネットワークを拡張することを想定している。
想定環境と類似した環境で実験を行うため、実験は最大で数十台のトンネル終端点という規模で行う。

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.65]{./img/experimenttopology}
		\caption{実験環境のトポロジー図}
		\label{img:experimenttopology}
	\end{center}
\end{figure}

実験環境に対する要求から、本実験を行うにあたり、6台の実機サーバーを利用して実験環境を構築した。この実験環境のトポロジー図を図~\ref{img:experimenttopology}に示す。
また、利用した実機サーバーの仕様を表~\ref{table:expserv}に示す。更に、それぞれのサーバーで利用したソフトウェアのバージョンを
表~\ref{table:expversion}に示す。
6台の実機サーバーの内、3台でLEONの実装であるleondを動作させた。このうち2台に、実際に拡張されたLayer 2ネットワークに参加する
ホストサーバーとして実機サーバーを直接接続した。さらに、残りの実機サーバー1台で最大27台の仮想マシンを動作させ、仮想マシン内部で
leondを動作させた。leondが動作するサーバーは全て同じLayer 2ネットワークに所属している。

\begin{table}[h]
\begin{center}
	\caption{実験で利用した実機サーバーの仕様}
	\begin{tabular}{|l|l|l|l|}
		\hline
		ホスト & CPU & メモリー & NIC \\
		\hline
		\hline
		Tunnel Server 1 & Intel Xeon L5520 2.27Ghz & 12GB & NetXtreme II BCM5709 \\
		\hline
		Tunnel Server 2 & Intel Xeon L5520 2.27Ghz & 24GB & Intel 82575EB \\
		\hline
		Tunnel Server 3 & Intel Xeon E5430 2.66Ghz & 4GB & NetXtreme II BCM5708 \\
		\hline
		Host A & Intel Xeon 5160 3.00Ghz & 4GB & Intel 80003ES2LAN \\
		\hline
		Host B & Intel Xeon 5160 3.00Ghz & 4GB & Intel 80003ES2LAN \\
		\hline
		VM Server & AMD Opteron 6128 & 20GB & Intel 82576 \\
		\hline
	\end{tabular}
	\label{table:expserv}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
	\caption{実験で利用した各サーバーのバージョン}
	\begin{tabular}{|l|l|l|l|}
		\hline
		サーバー & OS & Linux Kernel & gcc \\
		\hline
		\hline
		Tunnel Server 1 & Debian GNU/Linux 6.0.6 Squeeze & 3.7.2 & 4.4.5 \\
		Tunnel Server 2 & 				 &       &       \\
		Tunnel Server 3	&				 &       &       \\
		VM \#		&				 & 	 &	 \\
		\hline
		Host A 		& Debian GNU/Linux 6.0.6 Squeeze & 2.6.32-5 & 4.4.5 \\
		Host B		&				 &	    &       \\
		\hline
		VM Server 	& Ubuntu 10.04.4 LTS		 & 3.7.2 & 4.4.3 \\
		\hline
	\end{tabular}
	\label{table:expversion}
\end{center}
\end{table}

また、leondが動作するサーバー間で、tc(Linux Traffic Control)~\cite{tc}を利用して擬似的に遅延を発生させた。サーバー間に設定した遅延を
図~\ref{img:experimenttc}に示す。全てのトンネル終端点において、Tunnel Server 3への遅延は他のサーバーへの遅延よりも小さく設定した。
これにより、直接通信するよりも小さい遅延で通信することができる経路を作成した。全てのサーバー間の通信は
Tunnel Server 3を経由することで、直接通信するよりも小さい遅延で通信をすることができる。仮想マシン間には遅延を設定していない。

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.70]{./img/experimenttc}
		\caption{実験環境に設定された擬似的な遅延}
		\label{img:experimenttc}
	\end{center}
\end{figure}

これによって、実機サーバー6台を用いて、本実験を行うための要求を満たした実験環境を構築した。

\subsection{実験内容}
\label{experiment:spec}

本実験では、中継するトンネル終端点で障害が発生してから、全てのトンネル終端点でトポロジーが収束するまでの時間を計測する。
中継をするトンネル終端点で障害が発生すると、そのトンネル終端点を経由して行われていた通信は、トポロジーが収束するまで行えなくなる。
本実験では、この性質を利用してトポロジーの収束時間を計測するプログラムを作成した。本プログラムは、あるトンネル終端点から全てのトンネル終端点
に対して定期的にICMP Echo Requestを送信し、その返信を監視する。本プログラム開始後、中継を行なっているトンネル終端点
でネットワーク障害を発生させると、全てのトンネル終端点から返信がなくなる。返信がなくなると、中継するトンネル終端点で障害が発生したと判断し計測を開始する。
そして、全てのトンネル終端点でトポロジーが収束すると、全てのトンネル終端点から返信を受信できるようになる。本プログラムが全てのトンネル終端点から
返信を受け取ると、全てのトンネル終端点でトポロジーが収束したと判断し計測を終了する。

作成したプログラムはTunnel Server 1で動作させた。実験環境下でTunnel Server 1で動作するleondが構築する
Layer 2ネットワークのトポロジーと実験の様子を図~\ref{img:topologyat1}に示す。Tunnnel Server 1が、他のトンネル終端点へ
イーサネットフレームを転送する際には、必ずTunnel Server 3を経由する。作成したプログラムをTunnel Server 1で開始後、
Tunnel Server 3のネットワーク接続を切断した。Tunnel Server 3が切断されると、全てのトンネル終端点はTunnel Server 1と
通信ができなくなる。全てのトンネル終端点で動作するleondが、Tunnel Server 3での障害を検知すると、トポロジーが
再構築され、再びTunnel Server 1と通信ができるようになる。障害検知後のTunnel Server 1で動作するleondが構築する
トポロジーは図~\ref{img:topologyat1b}で示すようなトポロジーとなる。Tunnel Server 1と全てのトンネル終端点が
通信可能になった時点で計測終了である。

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.70]{./img/topologyat1}
		\caption{Tunnel Server 1で構築されるトポロジーと実験の様子}
		\label{img:topologyat1}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.70]{./img/topologyat1b}
		\caption{トポロジー収束後のトポロジー}
		\label{img:topologyat1b}
	\end{center}
\end{figure}

\subsection{実験結果}
\label{experiment:result}

本実験では、3台から30台までのトンネル終端点を利用して拡張したLayer 2ネットワークにおいて、中継を行なっているトンネル終端点
での障害発生後、全てのトンネル終端点でトポロジーが収束するまでにかかった時間を計測した。その実験結果を図~\ref{graph:topology}に
示す。本実験では、3台から30台までの各トンネル終端点数について、~\ref{experiment:spec}節で説明した計測を5回行った。計測を
行う度に全てのトンネル終端点でleondを再起動し、Layer 2ネットワークの再構築を行った。

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.80]{./img/topology}
		\caption{ノード数に応じたトポロジー収束時間}
		\label{graph:topology}
	\end{center}
\end{figure}

図~\ref{graph:topology}から、中継を行なっているトンネル終端点で障害が発生してから全てのトンネル終端点でトポロジーが収束
するまでにかかる時間は、同じトンネル終端点の台数でもばらつきが大きいことがわかる。25台のトンネル終端点では、トポロジーの収束にかかった時間
は最小で81秒、最大で123秒と42秒の差がある。一方で、同じ台数でもばらつきはあるが、各台数の平均値から、台数が増加するに連れトポロジーが収束するまでに
かかる時間の平均値は緩やかに増加していることがわかる。そのため、Layer 2ネットワークに参加しているトンネル終端点の台数が増加するに連れ、
中継を行なっているトンネル終端点で障害が発生してから全てのトンネル終端点でトポロジーが収束するまでにかかる時間は少しずつ増加する、と考えられる。

\subsection{考察}

本研究では、トンネル終端点の増加による影響を評価した。そして本評価では、Layer 2ネットワークに参加しているトンネル終端点の台数が、中継を行なっているトンネル終端点において障害
が発生してから全てのトンネル終端点でトポロジーが収束するまでかかる時間に与える影響を調査した。調査をするにあたり、
擬似的に遅延を発生させ、直接通信するよりも小さい遅延で通信できる経路が存在する実験環境を構築し、評価実験を行った。
その結果、同じトンネル終端点の台数でも、トポロジーが収束する時間のばらつきは大きいが、各台数の平均値からトンネル終端点の
台数が増加するに連れ、トポロジーの収束にかかる時間は緩やかに増加する、ということがわかった。よって、トンネル終端点の増加は
LEONに悪影響を及ぼすということがわかった。

この原因は、トンネル終端点を起動した時間の差であると考えられる。あるトンネル終端点で、トンネル終端点の障害発生からトポロジー
の再構築までかかる時間を$ T $秒、遅延計測を行う間隔時間を$ T_p $秒、最後に遅延計測を行なってから障害発生までに経過した時間を$ T_e $秒
とすると、これらの関係を次の数式で表すことができる。

\begin{displaymath}
\displaystyle T = (T_p - T_e) + T_p
\end{displaymath}

遅延計測を行う間隔時間(=$ T_p $)はデフォルト状態で60秒である。LEONは遅延計測が2回失敗すると障害発生と判断するため、障害発生からトポロジー
の再構築までかかる時間(=$ T $)は、$ T_p $秒以上となる。例えば、最後に遅延計測を行なってから20秒経過した時点で障害が発生すると、それを検知し
トポロジーの再構築までかかる時間は$ T = (60 - 20) + 60 = 100 $より100秒である。障害発生からトポロジーの再構築までかかる時間(=$ T $)は、最後に遅延計測を
行なってから障害発生までに経過した時間(=$ T_e $)が小さいほど大きくなる。
トンネル終端点が複数の場合、全てのトンネル終端点を同時に起動をしていないので、$ T_e $の値にはトンネル終端点ごとにばらつきがある。
トンネル終端点の台数が増えると$ T_e $の値のばらつきが大きくなり、最後に遅延計測を行なってから障害発生までに経過した時間が短いトンネル終端点がいる確率が高まる。
そのため、トンネル終端点の台数が増加するに連れ、中継を行なっているトンネル終端点において障害が発生してから全てのトンネル終端点でトポロジーが収束するまでかかる時間
が増加すると考えられる。

RFC 793ではTCPのタイムアウト時間は300秒と定義されている~\cite{rfc:tcp}。第~\ref{rw}章で挙げたLayer 2ネットワーク拡張技術を利用して構築されたLayer 2ネットワークで障害が発生すると、
管理者が障害の原因を特定し、修正するまでに時間がかかる。この手法では、障害が発生してから修正まで300秒以上かかり、TCPのセッションが切断されてしまう場合が多い。LEONでは、
中継を行なっているトンネル終端点で障害が発生してから、最大120秒で通信が再び可能となる。そのため、LEONを用いることで、障害発生時はTCPのセッションが切断される前に復旧
することができる。

\section{実験のまとめ}

本研究では、インターネット上に分散した複数の拠点を用いて構築されたサービスのパフォーマンス改善を目標としている。従来手法では、複数拠点を用いて
サービスを構築した場合、サービスを構成するコンポーネントのパフォーマンスが遅延により低下してしまうため、サービスのパフォーマンスが低下してしまう
という問題があった。この問題を解決するため、LEONは遅延の最も小さい経路でイーサネットフレームの転送を行う。これにより、コンポーネントのパフォーマンス
が改善され、サービスのパフォーマンスが改善されるということがわかった。

しかし、一方でトンネル終端点の台数が増加するとLEONに悪影響を及ぼすということがわかった。トンネル終端点の増加が与える影響として、中継を行うトンネル終端
点で障害が発生してから、そのトンネル終端点を経由していた通信の復旧までにかかる時間の増加があるということもわかった。これはLEONが分散して動作するために、
Layer 2ネットワークに参加している全てのトンネル終端点が全てのトンネル終端点の管理、死活監視や遅延計測などを行う必要があるためであると考える。そのため、
LEONは多くの拠点に同一のLayer 2ネットワークを拡張するためには適していないということがわかった。

%%% Local Variables:
%%% mode: japanese-latex
%%% TeX-master: "../yummy_bthesis"
%%% End:
